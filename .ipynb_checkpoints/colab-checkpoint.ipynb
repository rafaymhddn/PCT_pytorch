{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End 3D Detection & Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone --recursive https://github.com/rafaymhddn/PCT_pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd PCT_pytorch && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intsall Libs / Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\"\n",
    "!pip install open3d\n",
    "!pip install plotly\n",
    "!pip install ninja\n",
    "!pip install 'git+https://github.com/facebookresearch/pytorch3d.git@stable'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import subprocess\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vis import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    return {\n",
    "        'point_cloud': [item['point_cloud'] for item in batch],\n",
    "        'mask': [item['mask'] for item in batch],\n",
    "        'bbox3d': [item['bbox3d'] for item in batch], \n",
    "        'centroid': [item['centroid'] for item in batch]\n",
    "    }\n",
    "\n",
    "class PickPlaceDataset(Dataset):\n",
    "    def __init__(self, root_dir, sample_ids,  augment=False):\n",
    "        self.root_dir = root_dir\n",
    "        self.sample_ids = sample_ids\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_id = self.sample_ids[idx]\n",
    "        sample_path = os.path.join(self.root_dir, sample_id)\n",
    "\n",
    "        pc = np.load(os.path.join(sample_path, 'pc.npy'))        # [3, H, W] or [H, W, 3] ?\n",
    "        mask = np.load(os.path.join(sample_path, 'mask.npy'))    # [N, H, W]\n",
    "        bbox = np.load(os.path.join(sample_path, 'bbox3d.npy'))  #  [N, 8, 3]\n",
    "\n",
    "        H, W = pc.shape[1], pc.shape[2]\n",
    "        pc = pc.reshape(3, -1).transpose(1, 0)  # shape: [Points, 3]\n",
    "\n",
    "        # Reshape mask: [N, H, W] → [N, H*W]\n",
    "        mask = mask.reshape(mask.shape[0], -1)  # shape: [N, Points]\n",
    "\n",
    "        # Flip ?\n",
    "\n",
    "        if self.augment:\n",
    "            pc, bbox = self.apply_augmentations(pc, bbox)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return {\n",
    "            'point_cloud': pc.astype(np.float32),\n",
    "            'mask': mask.astype(np.int64),\n",
    "            'bbox3d': bbox.astype(np.float32),\n",
    "            'centroid': bbox.mean(axis=1).astype(np.float32)\n",
    "\n",
    "        }\n",
    "    \n",
    "    def apply_augmentations(self, pc, bbox):\n",
    "        \"\"\"\n",
    "        Applies a series of random augmentations to the point cloud and bounding box.\n",
    "        Assumes pc is [N, 3] and bbox is [N, 8, 3].\n",
    "        \"\"\"\n",
    "        # 1. Random Rotation around the Z-axis\n",
    "        angle = random.uniform(0, 2 * np.pi)\n",
    "        rotation_matrix = np.array([\n",
    "            [np.cos(angle), -np.sin(angle), 0],\n",
    "            [np.sin(angle),  np.cos(angle), 0],\n",
    "            [0,              0,             1]\n",
    "        ])\n",
    "        pc = pc @ rotation_matrix.T\n",
    "        bbox = bbox @ rotation_matrix.T\n",
    "\n",
    "        # 2. Random Scaling\n",
    "        scale = random.uniform(0.9, 1.1)\n",
    "        pc *= scale\n",
    "        bbox *= scale\n",
    "\n",
    "        # 3. Random Jitter (translation)\n",
    "        jitter = (np.random.rand(1, 3) - 0.5) * 0.02\n",
    "        pc += jitter\n",
    "        bbox += jitter\n",
    "        \n",
    "        return pc, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(data_root, batch_size=16, seed=42, train_size=0.8):\n",
    "        \n",
    "        all_sample_ids = sorted(os.listdir(data_root))\n",
    "        all_sample_ids = [s for s in all_sample_ids if os.path.isdir(os.path.join(data_root, s))]\n",
    "\n",
    "        random.seed(seed)\n",
    "        random.shuffle(all_sample_ids)\n",
    "\n",
    "        train_ids, temp_ids = train_test_split(\n",
    "        all_sample_ids, test_size=(1-train_size), random_state=seed\n",
    "        )\n",
    "    \n",
    "        # test and val split 50:50\n",
    "        val_ids, test_ids = train_test_split(\n",
    "        temp_ids, test_size=(0.5), random_state=seed\n",
    "        )\n",
    "\n",
    "        train_set = PickPlaceDataset(data_root, train_ids,  augment=True)\n",
    "        val_set = PickPlaceDataset(data_root, val_ids)\n",
    "        test_set = PickPlaceDataset(data_root, test_ids)\n",
    "\n",
    "        print(f\"Dataset sizes - Train: {len(train_set)}, Val: {len(val_set)}, Test: {len(test_set)}\")\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "        return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "def visualize_sample_plotly(pc, mask, bbox3d, show_background=False):\n",
    "    \"\"\"\n",
    "    Interactive Plotly visualization of 3D point cloud with instance masks, centroids, and 3D bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        pc: [P, 3] point cloud\n",
    "        mask: [N, P] binary masks\n",
    "        bbox3d: [N, 8, 3] bounding boxes with 8 corner points\n",
    "        show_background: bool, whether to display unlabeled points (background)\n",
    "    \"\"\"\n",
    "    pc = np.asarray(pc)\n",
    "    mask = np.asarray(mask)\n",
    "    bbox3d = np.asarray(bbox3d)\n",
    "\n",
    "    P = pc.shape[0]\n",
    "    N = mask.shape[0]\n",
    "\n",
    "    # Assign each point its instance ID\n",
    "    instance_ids = np.full((P,), -1)\n",
    "    for idx in range(N):\n",
    "        instance_ids[mask[idx] > 0] = idx\n",
    "\n",
    "    # Color map\n",
    "    colorscale = px.colors.qualitative.Dark24  # 24 distinct colors\n",
    "    num_colors = len(colorscale)\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add point cloud per instance\n",
    "    centroids = []\n",
    "    for i in range(N):\n",
    "        inds = np.where(instance_ids == i)[0]\n",
    "        if inds.size == 0:\n",
    "            centroids.append(np.array([np.nan, np.nan, np.nan]))\n",
    "            continue\n",
    "        pts = pc[inds]\n",
    "        cent = pts.mean(axis=0)\n",
    "        centroids.append(cent)\n",
    "\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(size=2, color=colorscale[i % num_colors]),\n",
    "            showlegend=False  # Remove legend entries for points\n",
    "        ))\n",
    "\n",
    "        # Add centroid point\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[cent[0]], y=[cent[1]], z=[cent[2]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=7, color='black', symbol='x'),\n",
    "            showlegend=False  # No legend for centroids\n",
    "        ))\n",
    "\n",
    "    centroids = np.vstack(centroids)  # Shape: [N, 3]\n",
    "\n",
    "    # Add unlabeled/background points if enabled\n",
    "    if show_background:\n",
    "        bg_inds = np.where(instance_ids == -1)[0]\n",
    "        if bg_inds.size > 0:\n",
    "            pts = pc[bg_inds]\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=pts[:, 0], y=pts[:, 1], z=pts[:, 2],\n",
    "                mode='markers',\n",
    "                marker=dict(size=1, color='gray', opacity=0.3),\n",
    "                showlegend=False  # Remove legend for background points\n",
    "            ))\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    edges = [\n",
    "        (0, 1), (1, 2), (2, 3), (3, 0),\n",
    "        (4, 5), (5, 6), (6, 7), (7, 4),\n",
    "        (0, 4), (1, 5), (2, 6), (3, 7)\n",
    "    ]\n",
    "\n",
    "    for i, box in enumerate(bbox3d):\n",
    "        for s, e in edges:\n",
    "            x, y, z = zip(box[s], box[e])\n",
    "            fig.add_trace(go.Scatter3d(\n",
    "                x=x, y=y, z=z,\n",
    "                mode='lines',\n",
    "                line=dict(color='black', width=2),\n",
    "                showlegend=False  # Remove legend for bbox lines\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title='3D Point Cloud with Instance Masks, Centroids, and BBoxes',\n",
    "        scene=dict(\n",
    "            xaxis_title='X', yaxis_title='Y', zaxis_title='Z',\n",
    "            aspectmode='data',\n",
    "        ),\n",
    "        showlegend=False  # Remove all legends\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_root = 'data/pick_place'\n",
    "train_loader, val_loader, test_loader = get_dataloaders(data_root)\n",
    "\n",
    "for batch in train_loader:\n",
    "        pc = batch['point_cloud']   # [Points, 3]\n",
    "        mask = batch['mask']        # [N, Points]\n",
    "        bbox = batch['bbox3d']      # [N, 8, 3] \n",
    "        centroid = batch['centroid']# [N, 3] \n",
    "\n",
    "        print(\"point_cloud shape:\", np.array(pc[0]).shape)\n",
    "        print(\"mask shape:\", np.array(mask[0]).shape)\n",
    "        print(\"bbox3d shape:\", np.array(bbox[0]).shape)\n",
    "        print(\"centroid shape:\", np.array(centroid[0]).shape)\n",
    "\n",
    "        visualize_sample_plotly(pc[0], mask[0], bbox[0])\n",
    "       \n",
    "    \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from module import Embedding, NeighborEmbedding, OA, SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class NaivePCT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = Embedding(3, 128)\n",
    "\n",
    "        self.sa1 = SA(128)\n",
    "        self.sa2 = SA(128)\n",
    "        self.sa3 = SA(128)\n",
    "        self.sa4 = SA(128)\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Conv1d(512, 1024, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x1 = self.sa1(x)\n",
    "        x2 = self.sa2(x1)\n",
    "        x3 = self.sa3(x2)\n",
    "        x4 = self.sa4(x3)\n",
    "        x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "\n",
    "        x = self.linear(x)\n",
    "\n",
    "        # x = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x_max = torch.max(x, dim=-1)[0]\n",
    "        x_mean = torch.mean(x, dim=-1)\n",
    "\n",
    "        return x, x_max, x_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation(nn.Module):\n",
    "    def __init__(self, part_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.part_num = part_num\n",
    "\n",
    "        self.label_conv = nn.Sequential(\n",
    "            nn.Conv1d(16, 64, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "\n",
    "        self.convs1 = nn.Conv1d(1024 * 3 + 64, 512, 1)\n",
    "        self.convs2 = nn.Conv1d(512, 256, 1)\n",
    "        self.convs3 = nn.Conv1d(256, self.part_num, 1)\n",
    "\n",
    "        self.bns1 = nn.BatchNorm1d(512)\n",
    "        self.bns2 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.dp1 = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, x, x_max, x_mean, cls_label):\n",
    "        batch_size, _, N = x.size()\n",
    "\n",
    "        x_max_feature = x_max.unsqueeze(-1).repeat(1, 1, N)\n",
    "        x_mean_feature = x_mean.unsqueeze(-1).repeat(1, 1, N)\n",
    "\n",
    "        cls_label_one_hot = cls_label.view(batch_size, 16, 1)\n",
    "        cls_label_feature = self.label_conv(cls_label_one_hot).repeat(1, 1, N)\n",
    "\n",
    "        x = torch.cat([x, x_max_feature, x_mean_feature, cls_label_feature], dim=1)  # 1024 * 3 + 64\n",
    "\n",
    "        x = F.relu(self.bns1(self.convs1(x)))\n",
    "        x = self.dp1(x)\n",
    "        x = F.relu(self.bns2(self.convs2(x)))\n",
    "        x = self.convs3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivePCTSeg(nn.Module):\n",
    "    def __init__(self, part_num=50):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.encoder = NaivePCT()\n",
    "        self.seg = Segmentation(part_num)\n",
    "\n",
    "    def forward(self, x, cls_label):\n",
    "        x, x_max, x_mean = self.encoder(x)\n",
    "        x = self.seg(x, x_max, x_mean, cls_label)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimplePointNet(nn.Module):\n",
    "    def __init__(self, feat_dim=64):\n",
    "        super(SimplePointNet, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        \n",
    "        # Point-wise feature extraction (shared MLP)\n",
    "        self.mlp1 = nn.Linear(3, 64)\n",
    "        self.mlp2 = nn.Linear(64, 128)\n",
    "        self.mlp3 = nn.Linear(128, feat_dim)\n",
    "\n",
    "        # Segmentation Head\n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),  # output: foreground mask probability\n",
    "        )\n",
    "\n",
    "        # Detection Head (centroid regression + bbox regression)\n",
    "        self.centroid_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3)  # centroid (x, y, z)\n",
    "        )\n",
    "\n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 24),  # 8 corners × 3 coords\n",
    "        )\n",
    "\n",
    "    def forward(self, pc):  # pc: [P, 3]\n",
    "        x = F.relu(self.mlp1(pc))\n",
    "        x = F.relu(self.mlp2(x))\n",
    "        features = self.mlp3(x)  # [P, feat_dim]\n",
    "\n",
    "        # Segmentation (instance-agnostic for now)\n",
    "        seg_logits = self.seg_head(features).squeeze(-1)  # [P]\n",
    "\n",
    "        # Global feature for detection (simple max pool)\n",
    "        global_feat = torch.max(features, dim=0, keepdim=True)[0]  # [1, feat_dim]\n",
    "\n",
    "        centroid_pred = self.centroid_head(global_feat).squeeze(0)  # [3]\n",
    "        bbox_pred = self.bbox_head(global_feat).view(8, 3)  # [8, 3]\n",
    "\n",
    "        return {\n",
    "            \"seg_logits\": seg_logits,       # [P]\n",
    "            \"centroid_pred\": centroid_pred, # [3]\n",
    "            \"bbox_pred\": bbox_pred          # [8, 3]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = {\n",
    "    'point_cloud': torch.randn(2048, 3),\n",
    "    'mask': torch.randint(0, 2, (4, 2048)),\n",
    "    'bbox3d': torch.randn(4, 8, 3),\n",
    "    'centroid': torch.randn(4, 3),\n",
    "}\n",
    "\n",
    "model = SimplePointNet()\n",
    "out = model(batch['point_cloud'])\n",
    "print(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing dimension\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Use MPS device if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate and move model to MPS\n",
    "model = SimplePointNet().to(device)\n",
    "model.eval()\n",
    "\n",
    "for batch in train_loader:\n",
    "    pc = batch['point_cloud']   # [B, Points, 3]\n",
    "    mask = batch['mask']        # [B, N, Points]\n",
    "    bbox = batch['bbox3d']      # [B, N, 8, 3]\n",
    "    centroid = batch['centroid']# [B, N, 3]\n",
    "\n",
    "    # Use only the first sample in the batch\n",
    "    pc_sample = pc[0]           # [Points, 3]\n",
    "    mask_sample = mask[0]       # [N, Points]\n",
    "    bbox_sample = bbox[0]       # [N, 8, 3]\n",
    "    centroid_sample = centroid[0] # [N, 3]\n",
    "\n",
    "    print(\"point_cloud shape:\", np.array(pc_sample).shape)\n",
    "    print(\"mask shape:\", np.array(mask_sample).shape)\n",
    "    print(\"bbox3d shape:\", np.array(bbox_sample).shape)\n",
    "    print(\"centroid shape:\", np.array(centroid_sample).shape)\n",
    "\n",
    "    # Optional: still visualize using NumPy\n",
    "    #visualize_sample_plotly(pc_sample, mask_sample, bbox_sample)\n",
    "\n",
    "    # ✅ Convert to Tensor and move to MPS device\n",
    "    pc_tensor = torch.tensor(pc_sample, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pc_tensor)\n",
    "\n",
    "    print(\"segmentation logits shape:\", outputs['seg_logits'].shape)\n",
    "    print(\"predicted centroid:\", outputs['centroid_pred'].cpu().numpy())\n",
    "    print(\"predicted bbox shape:\", outputs['bbox_pred'].cpu().numpy())\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over fit to one instance of example\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Device setup (MPS if available, else CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate model and move to device\n",
    "model = SimplePointNet().to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Get one batch/sample from train_loader\n",
    "for batch in train_loader:\n",
    "    pc = batch['point_cloud'][0]     # [Points, 3]\n",
    "    mask = batch['mask'][0]          # [N, Points]\n",
    "    bbox = batch['bbox3d'][0]        # [N, 8, 3]\n",
    "    centroid = batch['centroid'][0]  # [N, 3]\n",
    "    break  # only one batch/sample\n",
    "\n",
    "# Convert inputs & GT to tensors on device\n",
    "pc_tensor = torch.tensor(pc, dtype=torch.float32).to(device)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32).to(device)   # shape: [N, Points]\n",
    "bbox_tensor = torch.tensor(bbox, dtype=torch.float32).to(device)   # shape: [N, 8, 3]\n",
    "centroid_tensor = torch.tensor(centroid, dtype=torch.float32).to(device) # [N, 3]\n",
    "\n",
    "# For simplicity, assume N=1 (one instance) or pick first instance to overfit\n",
    "mask_single = mask_tensor[0]      # [Points]\n",
    "bbox_single = bbox_tensor[0]      # [8,3]\n",
    "centroid_single = centroid_tensor[0]  # [3]\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(pc_tensor)  # outputs: dict with keys 'seg_logits', 'centroid_pred', 'bbox_pred'\n",
    "\n",
    "    # seg_logits: [Points] raw scores for binary segmentation (point belongs to object or not)\n",
    "    seg_logits = outputs['seg_logits']  # [Points]\n",
    "    centroid_pred = outputs['centroid_pred']  # [3]\n",
    "    bbox_pred = outputs['bbox_pred']  # [8,3]\n",
    "\n",
    "    # Segmentation loss (Binary Cross Entropy)\n",
    "    seg_loss = F.binary_cross_entropy_with_logits(seg_logits, mask_single)\n",
    "\n",
    "    # Centroid loss (L2)\n",
    "    centroid_loss = F.mse_loss(centroid_pred, centroid_single)\n",
    "\n",
    "    # Bounding box loss (L2)\n",
    "    bbox_loss = F.mse_loss(bbox_pred, bbox_single)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = seg_loss + centroid_loss + bbox_loss\n",
    "\n",
    "    # Backpropagation\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"Finished training to overfit one example.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Make sure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# Convert example data to tensors on device (reuse from training)\n",
    "pc_tensor = torch.tensor(pc, dtype=torch.float32).to(device)\n",
    "mask_tensor = torch.tensor(mask, dtype=torch.float32).to(device)\n",
    "bbox_tensor = torch.tensor(bbox, dtype=torch.float32).to(device)\n",
    "centroid_tensor = torch.tensor(centroid, dtype=torch.float32).to(device)\n",
    "\n",
    "# Pick first instance (index 0)\n",
    "mask_single = mask_tensor[0]      # [Points]\n",
    "bbox_single = bbox_tensor[0]      # [8,3]\n",
    "centroid_single = centroid_tensor[0]  # [3]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(pc_tensor)\n",
    "\n",
    "    seg_logits = outputs['seg_logits']  # [Points]\n",
    "    centroid_pred = outputs['centroid_pred']  # [3]\n",
    "    bbox_pred = outputs['bbox_pred']  # [8, 3]\n",
    "\n",
    "    # Convert segmentation logits to probabilities\n",
    "    seg_probs = torch.sigmoid(seg_logits)\n",
    "\n",
    "    # Binarize mask prediction (threshold=0.5)\n",
    "    seg_pred = (seg_probs > 0.5).float()\n",
    "\n",
    "# Move tensors back to CPU & numpy for printing/comparison\n",
    "seg_pred_np = seg_pred.cpu().numpy()\n",
    "seg_gt_np = mask_single.cpu().numpy()\n",
    "\n",
    "centroid_pred_np = centroid_pred.cpu().numpy()\n",
    "centroid_gt_np = centroid_single.cpu().numpy()\n",
    "\n",
    "bbox_pred_np = bbox_pred.cpu().numpy()\n",
    "bbox_gt_np = bbox_single.cpu().numpy()\n",
    "\n",
    "# Print comparison metrics\n",
    "print(\"Segmentation Accuracy: {:.4f}\".format((seg_pred_np == seg_gt_np).mean()))\n",
    "print(\"Centroid Prediction (pred vs GT):\")\n",
    "print(centroid_pred_np)\n",
    "print(centroid_gt_np)\n",
    "print(\"Bounding Box Prediction vs GT (first 3 points):\")\n",
    "print(bbox_pred_np[:3])\n",
    "print(bbox_gt_np[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Ground Truth:\")\n",
    "visualize_sample_plotly(pc, mask, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Visualizing Prediction:\")\n",
    "# Make predicted bbox shape [1, 8, 3] for visualize_sample_plotly\n",
    "bbox_pred_np_exp = np.expand_dims(bbox_pred_np, axis=0)\n",
    "\n",
    "visualize_sample_plotly(pc, seg_pred_np, bbox_pred_np_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Device setup (MPS if available, else CPU)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Define fixed number of points per cloud\n",
    "NUM_POINTS = 100000\n",
    "\n",
    "# Sample or pad a point cloud or mask to NUM_POINTS\n",
    "def sample_points(x, num_points=NUM_POINTS):\n",
    "    x = torch.tensor(x)\n",
    "    N = x.shape[0]\n",
    "    if N >= num_points:\n",
    "        idx = torch.randperm(N)[:num_points]\n",
    "        return x[idx]\n",
    "    else:\n",
    "        pad_idx = torch.randint(0, N, (num_points - N,))\n",
    "        return torch.cat([x, x[pad_idx]], dim=0)\n",
    "\n",
    "# Instantiate model and move to device\n",
    "model = SimplePointNet().to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Load a batch from train_loader\n",
    "for batch in train_loader:\n",
    "    pc_batch = batch['point_cloud']     # list of [P_i, 3]\n",
    "    mask_batch = batch['mask']          # list of [P_i]\n",
    "    bbox_batch = batch['bbox3d']        # tensor [B, 8, 3]\n",
    "    centroid_batch = batch['centroid']  # tensor [B, 3]\n",
    "    break  # take one batch only\n",
    "\n",
    "# Sample each point cloud and corresponding mask\n",
    "pc_tensor = torch.stack([\n",
    "    sample_points(pc, NUM_POINTS) for pc in pc_batch\n",
    "]).float().to(device)  # [B, NUM_POINTS, 3]\n",
    "\n",
    "mask_tensor = torch.stack([\n",
    "    sample_points(mask, NUM_POINTS) for mask in mask_batch\n",
    "]).float().to(device)  # [B, NUM_POINTS]\n",
    "\n",
    "# Convert bbox and centroid directly\n",
    "bbox_tensor = torch.tensor(bbox_batch, dtype=torch.float32).to(device)         # [B, 8, 3]\n",
    "centroid_tensor = torch.tensor(centroid_batch, dtype=torch.float32).to(device) # [B, 3]\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(pc_tensor)  # dict with keys: seg_logits, centroid_pred, bbox_pred\n",
    "\n",
    "    seg_logits = outputs['seg_logits']        # [B, NUM_POINTS]\n",
    "    centroid_pred = outputs['centroid_pred']  # [B, 3]\n",
    "    bbox_pred = outputs['bbox_pred']          # [B, 8, 3]\n",
    "\n",
    "    # Losses\n",
    "    seg_loss = F.binary_cross_entropy_with_logits(seg_logits, mask_tensor)\n",
    "    centroid_loss = F.mse_loss(centroid_pred, centroid_tensor)\n",
    "    bbox_loss = F.mse_loss(bbox_pred, bbox_tensor)\n",
    "\n",
    "    total_loss = seg_loss + centroid_loss + bbox_loss\n",
    "    print(f\"Total loss: {total_loss}\")\n",
    "\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "print(\"Finished training to overfit batch of examples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
